# Batch Inference Example

For information about stream inference testing, refer to [Stream Inference](streaming_inference.md).

## Example Flow

An example of the general flow is shown below. Ensure that the user:

1. Has acquired the dataset and the trained model.
1. Converts a dataset into _Serialized Protocol Buffers_ (PBs). Refer to https://developers.google.com/protocol-buffers/ for additional PB information.
1. Invokes `nctl mount`.
1. Mounts the Samba shared folder by invoking the command displayed in the previous step.
1. Copies the serialized PBs and the trained model to the just-mounted share.
1. Runs `nctl predict batch` command.

## MNIST Example

You need to have preprocessed MNIST data for feeding the batch inference. You can generate example data executing the following steps:

### MNIST Data Preprocessing

1. Create venv by executing the following command:

   ```
   $ python3 -m venv .venv
   ```

1. Install the required dependency in venv:

   ```
   $ source .venv/bin/activate
   $ pip install tensorflow-serving-api
   ```

1. Run the `mnist_converter_pb.py` (from: `nauta/applications/cli/example-python/package_examples`) using just-generated venv:

   ```
   $ python mnist_converter_pb.py
   ```

### Start Prediction

1. Mount Samba input share to your directory (use `/mnt/input` for mounting). Then, execute the command printed by 
`nctl mount`.

1. Copy the model generated by: `trained_mnist_model` to: `/mnt/input`.

1. Execute the following command: 

`$ nctl predict batch --model-location /mnt/input/home/trained_mnist_model --data /mnt/input/home/parsed --model-name mnist`

### Other Important Information

* **Paths:** Paths provided in locations such as, `--model-location` and `--data` need to point (for files/directory) from the container's context, _not_ from a user's filesystem or mounts. These paths can be mapped using instructions from `nctl mount`. 

   For example, if you have mounted Samba `/input` and copied the files there, you should pass: `/mnt/input/home/<file>`.

* **Model Name:** The`--model-name` is optional, but it _must_ match the model name provided during data preprocessing, since generated requests _must_ define which servable they target. In the `mnist_converter_pb.py` script, you can find 
`request.model_spec.name = 'mnist'`. This saves the model name in requests, and that name _must_ match a value passed as: 
`--model-name`

    If not provided, it assumes that the model name is equal to last directory in model location:
`/mnt/input/home/trained_mnist_model` --> `trained_mnist_model`

## References

* https://www.tensorflow.org/serving/serving_basic
* https://developers.google.com/protocol-buffers/docs/pythontutorial
* https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_client.py
* https://www.tensorflow.org/serving/docker




