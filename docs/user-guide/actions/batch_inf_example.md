# Batch Inference Example

For information about stream inference testing, refer to [Stream Inference](streaming_inference.md).

This section discusses the following main topics: 

 - [Flow Example](#flow-example)
 - [MNIST Example](#mnist-example)  
 - [MNIST Data Preprocessing](#mnist-data-preprocessing)
 - [Start Prediction](#start-prediction)
 - [Other Important Information](#other-important-information)
 - [Useful References](#useful-references)

## Flow Example

An example of the general flow is shown below. Ensure that you have:

1. Acquired the dataset and the trained model.

1. Converted a dataset into _Serialized Protocol Buffers_ (PBs). Refer to [Protocol Buffers](https://developers.google.com/protocol-buffers) for additional PB information.

1. Invoked `nctl mount`.

1. Mounted the Samba shared folder by invoking the `nctl mount`command (in step 3).

1. Copied the serialized PBs and the trained model to the just-mounted shared folder.

1. Run `nctl predict batch` command.

If the general flow requirements _are not_ met, the user _will not_ be able to complete the example. 

## MNIST Example

You need to have preprocessed MNIST data for feeding the batch inference. You can generate example data by performing the following steps:

## MNIST Data Preprocessing

1. Create venv by executing the following command:

   ```
   python3 -m venv .venv
   ```

1. Install required dependency in venv:
   ```
   ource .venv/bin/activate
   pip install tensorflow-serving-api
   ```

1. Run the `mnist_converter_pb.py` using just-generated venv:
   ```
   python mnist_converter_pb.py
   ```

### Parameters of mnist_converter_pb.py

* `work_dir` - Location where files related with conversion will be stored. The default is: `/tmp/mnist_tests`.

* `num_tests` - Number of examples to convert.  Default: `100`.

**Note**: Results of conversion are stored in `conversion_out` directory under `work_dir` parameter. The default is: `/tmp/mnist_tests/conversion_out`.

## Start Prediction

1. Mount Samba input share to your directory, assumed `/mnt/input` . 

1. Then, execute the command printed by:

  ```
  nctl mount
  ```

1. Copy model generated by: `trained_mnist_model` to: `/mnt/input`.

1. Execute the following command (scroll right to see full contents): 

  ```
  nctl predict batch --model-location /mnt/input/home/trained_mnist_model --data /mnt/input/home/parsed --model-name mnist
  ```
## Other Important Information

### Paths 

Paths provided in locations such as, `--model-location` and `--data` need to point (for `files/directory`) from the container's context, _**not**_ from a user's filesystem or mounts. These paths can be mapped using instructions from `nctl mount`. 

For example, if you have mounted Samba `/input` and copied the files there, you should pass:

`/mnt/input/home/<file>`

### Model Name

The `--model-name` is optional, but it _must_ match the model name provided during data preprocessing, since generated requests _must_ define which servable they target. 

In the `mnist_converter_pb.py` script, you can find 
`request.model_spec.name = 'mnist'`. This saves the model name in requests, and that name _must_ match a value passed as: 
`--model-name`

If _not_ provided, it assumes that the model name is equal to last directory in model location:
`/mnt/input/home/trained_mnist_model` --> `trained_mnist_model`

## Useful References

* [Serving a TensorFlow Model](https://www.tensorflow.org/serving/serving_basic)
* [Protocol Buffer Basics: Python](https://developers.google.com/protocol-buffers/docs/pythontutorial)
* [mnist_client.py Script](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_client.py)
* [TensorFlow Serving with Docker](https://www.tensorflow.org/serving/docker)

----------------------

## Return to Start of Document

* [README](../README.md)
----------------------

